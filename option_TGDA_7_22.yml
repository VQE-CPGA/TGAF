dataset:
  train:  # LMDB
    type: MFQEv2Dataset
    
    # for create lmdb
    root: /home/zhuqiang/STDF30/data/MFQEv2/
    gt_folder: train_108/raw/
    lq_folder: train_108/HM16.5_LDP/QP22/

    # for dataset
    gt_path: mfqev2_train_gt.lmdb
    lq_path: mfqev2_stdf_lq.lmdb
    meta_info_fp: meta_info.txt
    gt_size: 128  # ground truth patch size: gt_size * gt_size
    use_flip: True
    use_rot: True  # rotation per 90 degrees
    random_reverse: False
    # for datasampler
    enlarge_ratio: 300  # enlarge dataset by randomly cropping.
    
    # for dataloader
    num_worker_per_gpu: 18  # 12 in total. mainly affect IO speed
    batch_size_per_gpu: 32  # bs=32, divided by 4 GPUs

  test:  # Disk IO
    type: VideoTestMFQEv2Dataset
    root: /home/zhuqiang/STDF30/data/MFQEv2/
    gt_path: test_18/raw/
    lq_path: test_18/HM16.5_LDP/QP22/


  val:  # Disk IO
    type: VideoTestMFQEv2Dataset
    root: /home/zhuqiang/STDF30/data/MFQEv2/
    # gt_path: test_02/raw/
    # lq_path: test_02/HM16.5_LDP/QP22/
    gt_path: test_00/raw/
    lq_path: test_00/HM16.5_LDP/QP22/

network:
  radius: 3  # total num of input frame = 2 * radius + 1

  stdf:
    in_nc: 1  # 1 for Y
    out_nc: 64
    nf: 64  # num of feature maps
    nb: 3  # num of conv layers
    base_ks: 3
    deform_ks: 3  # size of the deformable kernel
  
  qenet:
    netname: default 
    method: replace
    in_nc: 64  # = out_nc of stdf
    out_nc: 1  # 1 for Y
    nf: 64
    nb: 5
    base_ks: 3

    att: True
    attname: DSTA

train:
  is_dist: False # True
  exp_name: TGDA_7_QP22_MFQEv2 # default: timestr. None: ~
  random_seed: 233
  pre-val: False  # evaluate criterion before training, e.g., ori PSNR
  num_iter: !!float 10e+5
  interval_print: !!float 100
  # interval_val: !!float 5e+3  # 10e+3  # also save model
  interval_val: !!float  5e+3  # 100  # also save model
  pbar_len: 100

  optim:
    type: Adam
    lr: !!float 1e-4  # init lr of scheduler
    betas: [0.9, 0.999]
    eps: !!float 1e-08

  scheduler:
    # is_on: True
    # type: MultiStepLR
    # milestones: [0.8,0.9]
    # gamma: 0.5
    is_on: False
    type: CosineAnnealingRestartLR
    periods: [!!float 5e+4, !!float 5e+4, !!float 5e+4, !!float 5e+4, !!float 5e+4, !!float 5e+4]  # epoch interval
    restart_weights: [1, 0.5, 0.5, 0.5, 0.5, 0.5]
    eta_min: !!float 1e-7

  loss:
    type: CharbonnierLoss
    eps: !!float 1e-6

  criterion:
    type: PSNR
    unit: dB

test:
  restore_iter: !!float 375000 # 955000 # 225000 # 110000 # 60000 # 500000 # 480000 # 365000 # 225000 # 285000 # 260000 # 220000 # 260000 # 410000 # 260000 #  365000 # 500000 # 445000 #  410000 # 395000 # 380000 # 225000 # 225000 # 485000 # 480000 #  10000 # 250000 
  pbar_len: 100
  checkpoint_save_path: 'exp/TGDA_QP22_MFQEv2/TGDA_QP22.pt' 
  criterion:
    type: PSNR
    unit: dB